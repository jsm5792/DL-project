{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a",
      "metadata": {
        "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a"
      },
      "source": [
        "## Copyright 2022 Google LLC. Double-click for license information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5",
      "metadata": {
        "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ka458jMeyAQA",
      "metadata": {
        "id": "Ka458jMeyAQA"
      },
      "source": [
        "# 버전 관리를 위한 코드들"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "miFwy5n7yC71",
      "metadata": {
        "id": "miFwy5n7yC71"
      },
      "outputs": [],
      "source": [
        "# 깔끔히 정리\n",
        "!pip -q uninstall -y diffusers transformers tokenizers accelerate xformers\n",
        "\n",
        "# diffusers는 P2P 시절 버전 유지\n",
        "!pip -q install --no-deps \"diffusers==0.3.0\" ftfy opencv-python ipywidgets\n",
        "\n",
        "# Py3.12에서 호환되는 짝\n",
        "!pip -q install \"transformers==4.38.2\" \"tokenizers==0.15.2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wUnZ10mqtp9L",
      "metadata": {
        "id": "wUnZ10mqtp9L"
      },
      "outputs": [],
      "source": [
        "import torch, diffusers, transformers, tokenizers\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"diffusers:\", diffusers.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"tokenizers:\", tokenizers.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2b343b-1c90-4747-a753-71eb7071a289",
      "metadata": {
        "id": "6d2b343b-1c90-4747-a753-71eb7071a289"
      },
      "source": [
        "# Null-text inversion + Editing with Prompt-to-Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
      "metadata": {
        "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "import torch.nn.functional as nnf\n",
        "import numpy as np\n",
        "import abc\n",
        "import ptp_utils\n",
        "import seq_aligner\n",
        "import shutil\n",
        "from torch.optim.adam import Adam\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7",
      "metadata": {
        "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7"
      },
      "source": [
        "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update MY_TOKEN with your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1",
      "metadata": {
        "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1"
      },
      "outputs": [],
      "source": [
        "# --- 수동 로드로 파이프라인 구성 (diffusers==0.3.0 호환, 안전하게 전체 모듈 명시) ---\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPFeatureExtractor  # 구버전 호환\n",
        "from diffusers import (\n",
        "    AutoencoderKL, UNet2DConditionModel,\n",
        "    PNDMScheduler, DDIMScheduler, StableDiffusionPipeline\n",
        ")\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "\n",
        "repo = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# 1) CLIP 토크나이저/텍스트 인코더\n",
        "tokenizer    = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# 2) UNet / VAE / (기본) PNDM 스케줄러 - 레포 하위폴더에서 직접 로드\n",
        "unet      = UNet2DConditionModel.from_pretrained(repo, subfolder=\"unet\")\n",
        "vae       = AutoencoderKL.from_pretrained(repo, subfolder=\"vae\")\n",
        "scheduler = PNDMScheduler.from_config(repo, subfolder=\"scheduler\")\n",
        "\n",
        "# 3) safety_checker / feature_extractor 도 '실제 객체'로 로드 (diffusers==0.3.0은 None/생략 불가)\n",
        "safety_checker    = StableDiffusionSafetyChecker.from_pretrained(repo, subfolder=\"safety_checker\")\n",
        "feature_extractor = CLIPFeatureExtractor.from_pretrained(repo, subfolder=\"feature_extractor\")\n",
        "\n",
        "# 4) 파이프라인 조립\n",
        "ldm_stable = StableDiffusionPipeline(\n",
        "    text_encoder=text_encoder,\n",
        "    vae=vae,\n",
        "    unet=unet,\n",
        "    tokenizer=tokenizer,\n",
        "    scheduler=scheduler,              # 일단 PNDM으로\n",
        "    safety_checker=safety_checker,\n",
        "    feature_extractor=feature_extractor,\n",
        ").to(device)\n",
        "\n",
        "# 5) DDIM 스케줄러로 교체 (레포에서 로드 후, 파라미터만 덮어쓰기)\n",
        "ddim = DDIMScheduler.from_config(repo, subfolder=\"scheduler\")  # 여기서 레포/폴더를 넘겨야 함 (FrozenDict X)\n",
        "# 원 노트북 파라미터와 동일하게 세팅\n",
        "ddim.beta_start       = 0.00085\n",
        "ddim.beta_end         = 0.012\n",
        "ddim.beta_schedule    = \"scaled_linear\"\n",
        "ddim.clip_sample      = False\n",
        "ddim.set_alpha_to_one = False\n",
        "\n",
        "ldm_stable.scheduler = ddim  # 최종 교체\n",
        "\n",
        "# (선택) xformers 최적화 끄기 (없으면 조용히 패스)\n",
        "try:\n",
        "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "# 원본 노트북 변수명 유지\n",
        "tokenizer = ldm_stable.tokenizer\n",
        "\n",
        "print(\"Loaded:\", type(ldm_stable.unet).__name__, \"| Scheduler:\", type(ldm_stable.scheduler).__name__)\n",
        "print(\"beta_start/end:\", ldm_stable.scheduler.beta_start, ldm_stable.scheduler.beta_end)\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWuI5aLjHviJ",
      "metadata": {
        "id": "gWuI5aLjHviJ"
      },
      "outputs": [],
      "source": [
        "NUM_DDIM_STEPS = 50\n",
        "LOW_RESOURCE = False\n",
        "GUIDANCE_SCALE = 7.5\n",
        "MAX_NUM_WORDS = 77"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01422991-cafe-4cf0-8406-66f052a75d9b",
      "metadata": {
        "id": "01422991-cafe-4cf0-8406-66f052a75d9b"
      },
      "source": [
        "## Prompt-to-Prompt code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc083590-15de-4216-8d8d-50336f9f1d34",
      "metadata": {
        "id": "dc083590-15de-4216-8d8d-50336f9f1d34"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LocalBlend:\n",
        "\n",
        "    def get_mask(self, maps, alpha, use_pool):\n",
        "        k = 1\n",
        "        maps = (maps * alpha).sum(-1).mean(1)\n",
        "        if use_pool:\n",
        "            maps = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
        "        mask = nnf.interpolate(maps, size=(x_t.shape[2:]))\n",
        "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
        "        mask = mask.gt(self.th[1-int(use_pool)])\n",
        "        mask = mask[:1] + mask\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, x_t, attention_store):\n",
        "        self.counter += 1\n",
        "        if self.counter > self.start_blend:\n",
        "\n",
        "            maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
        "            maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
        "            maps = torch.cat(maps, dim=1)\n",
        "            mask = self.get_mask(maps, self.alpha_layers, True)\n",
        "            if self.substruct_layers is not None:\n",
        "                maps_sub = ~self.get_mask(maps, self.substruct_layers, False)\n",
        "                mask = mask * maps_sub\n",
        "            mask = mask.float()\n",
        "            x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
        "        return x_t\n",
        "\n",
        "    def __init__(self, prompts: List[str], words: [List[List[str]]], substruct_words=None, start_blend=0.2, th=(.3, .3)):\n",
        "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
        "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
        "            if type(words_) is str:\n",
        "                words_ = [words_]\n",
        "            for word in words_:\n",
        "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
        "                alpha_layers[i, :, :, :, :, ind] = 1\n",
        "\n",
        "        if substruct_words is not None:\n",
        "            substruct_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
        "            for i, (prompt, words_) in enumerate(zip(prompts, substruct_words)):\n",
        "                if type(words_) is str:\n",
        "                    words_ = [words_]\n",
        "                for word in words_:\n",
        "                    ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
        "                    substruct_layers[i, :, :, :, :, ind] = 1\n",
        "            self.substruct_layers = substruct_layers.to(device)\n",
        "        else:\n",
        "            self.substruct_layers = None\n",
        "        self.alpha_layers = alpha_layers.to(device)\n",
        "        self.start_blend = int(start_blend * NUM_DDIM_STEPS)\n",
        "        self.counter = 0\n",
        "        self.th=th\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EmptyControl:\n",
        "\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        return x_t\n",
        "\n",
        "    def between_steps(self):\n",
        "        return\n",
        "\n",
        "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        return attn\n",
        "\n",
        "\n",
        "class AttentionControl(abc.ABC):\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        return x_t\n",
        "\n",
        "    def between_steps(self):\n",
        "        return\n",
        "\n",
        "    @property\n",
        "    def num_uncond_att_layers(self):\n",
        "        return self.num_att_layers if LOW_RESOURCE else 0\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
        "            if LOW_RESOURCE:\n",
        "                attn = self.forward(attn, is_cross, place_in_unet)\n",
        "            else:\n",
        "                h = attn.shape[0]\n",
        "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
        "        self.cur_att_layer += 1\n",
        "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
        "            self.cur_att_layer = 0\n",
        "            self.cur_step += 1\n",
        "            self.between_steps()\n",
        "        return attn\n",
        "\n",
        "    def reset(self):\n",
        "        self.cur_step = 0\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cur_step = 0\n",
        "        self.num_att_layers = -1\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "class SpatialReplace(EmptyControl):\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        if self.cur_step < self.stop_inject:\n",
        "            b = x_t.shape[0]\n",
        "            x_t = x_t[:1].expand(b, *x_t.shape[1:])\n",
        "        return x_t\n",
        "\n",
        "    def __init__(self, stop_inject: float):\n",
        "        super(SpatialReplace, self).__init__()\n",
        "        self.stop_inject = int((1 - stop_inject) * NUM_DDIM_STEPS)\n",
        "\n",
        "\n",
        "class AttentionStore(AttentionControl):\n",
        "\n",
        "    @staticmethod\n",
        "    def get_empty_store():\n",
        "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
        "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
        "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
        "            self.step_store[key].append(attn)\n",
        "        return attn\n",
        "\n",
        "    def between_steps(self):\n",
        "        if len(self.attention_store) == 0:\n",
        "            self.attention_store = self.step_store\n",
        "        else:\n",
        "            for key in self.attention_store:\n",
        "                for i in range(len(self.attention_store[key])):\n",
        "                    self.attention_store[key][i] += self.step_store[key][i]\n",
        "        self.step_store = self.get_empty_store()\n",
        "\n",
        "    def get_average_attention(self):\n",
        "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
        "        return average_attention\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        super(AttentionStore, self).reset()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AttentionStore, self).__init__()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "\n",
        "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        if self.local_blend is not None:\n",
        "            x_t = self.local_blend(x_t, self.attention_store)\n",
        "        return x_t\n",
        "\n",
        "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
        "        if att_replace.shape[2] <= 32 ** 2:\n",
        "            attn_base = attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
        "            return attn_base\n",
        "        else:\n",
        "            return att_replace\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
        "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
        "            h = attn.shape[0] // (self.batch_size)\n",
        "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
        "            attn_base, attn_repalce = attn[0], attn[1:]\n",
        "            if is_cross:\n",
        "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
        "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
        "                attn[1:] = attn_repalce_new\n",
        "            else:\n",
        "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce, place_in_unet)\n",
        "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
        "        return attn\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int,\n",
        "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
        "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
        "                 local_blend: Optional[LocalBlend]):\n",
        "        super(AttentionControlEdit, self).__init__()\n",
        "        self.batch_size = len(prompts)\n",
        "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
        "        if type(self_replace_steps) is float:\n",
        "            self_replace_steps = 0, self_replace_steps\n",
        "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
        "        self.local_blend = local_blend\n",
        "\n",
        "class AttentionReplace(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
        "\n",
        "\n",
        "class AttentionRefine(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
        "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
        "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
        "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
        "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
        "\n",
        "\n",
        "class AttentionReweight(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        if self.prev_controller is not None:\n",
        "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
        "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
        "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
        "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
        "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.equalizer = equalizer.to(device)\n",
        "        self.prev_controller = controller\n",
        "\n",
        "\n",
        "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
        "                  Tuple[float, ...]]):\n",
        "    if type(word_select) is int or type(word_select) is str:\n",
        "        word_select = (word_select,)\n",
        "    equalizer = torch.ones(1, 77)\n",
        "\n",
        "    for word, val in zip(word_select, values):\n",
        "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
        "        equalizer[:, inds] = val\n",
        "    return equalizer\n",
        "\n",
        "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
        "    out = []\n",
        "    attention_maps = attention_store.get_average_attention()\n",
        "    num_pixels = res ** 2\n",
        "    for location in from_where:\n",
        "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
        "            if item.shape[1] == num_pixels:\n",
        "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
        "                out.append(cross_maps)\n",
        "    out = torch.cat(out, dim=0)\n",
        "    out = out.sum(0) / out.shape[0]\n",
        "    return out.cpu()\n",
        "\n",
        "\n",
        "def make_controller(prompts: List[str], is_replace_controller: bool, cross_replace_steps: Dict[str, float], self_replace_steps: float, blend_words=None, equilizer_params=None) -> AttentionControlEdit:\n",
        "    if blend_words is None:\n",
        "        lb = None\n",
        "    else:\n",
        "        lb = LocalBlend(prompts, blend_word)\n",
        "    if is_replace_controller:\n",
        "        controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
        "    else:\n",
        "        controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
        "    if equilizer_params is not None:\n",
        "        eq = get_equalizer(prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"])\n",
        "        controller = AttentionReweight(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps,\n",
        "                                       self_replace_steps=self_replace_steps, equalizer=eq, local_blend=lb, controller=controller)\n",
        "    return controller\n",
        "\n",
        "\n",
        "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
        "    tokens = tokenizer.encode(prompts[select])\n",
        "    decoder = tokenizer.decode\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
        "    images = []\n",
        "    for i in range(len(tokens)):\n",
        "        image = attention_maps[:, :, i]\n",
        "        image = 255 * image / image.max()\n",
        "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
        "        image = image.numpy().astype(np.uint8)\n",
        "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
        "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.stack(images, axis=0))\n",
        "\n",
        "\n",
        "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
        "                        max_com=10, select: int = 0):\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
        "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
        "    images = []\n",
        "    for i in range(max_com):\n",
        "        image = vh[i].reshape(res, res)\n",
        "        image = image - image.min()\n",
        "        image = 255 * image / image.max()\n",
        "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
        "        image = Image.fromarray(image).resize((256, 256))\n",
        "        image = np.array(image)\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.concatenate(images, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a914bda0-c191-4db6-b891-101cde74ddaf",
      "metadata": {
        "id": "a914bda0-c191-4db6-b891-101cde74ddaf"
      },
      "source": [
        "## Null Text Inversion code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c442992d-8156-4dfc-a2a5-1fbf8bedb4b2",
      "metadata": {
        "id": "c442992d-8156-4dfc-a2a5-1fbf8bedb4b2"
      },
      "outputs": [],
      "source": [
        "def load_512(image_path, left=0, right=0, top=0, bottom=0):\n",
        "    if type(image_path) is str:\n",
        "        image = np.array(Image.open(image_path))[:, :, :3]\n",
        "    else:\n",
        "        image = image_path\n",
        "    h, w, c = image.shape\n",
        "    left = min(left, w-1)\n",
        "    right = min(right, w - left - 1)\n",
        "    top = min(top, h - left - 1)\n",
        "    bottom = min(bottom, h - top - 1)\n",
        "    image = image[top:h-bottom, left:w-right]\n",
        "    h, w, c = image.shape\n",
        "    if h < w:\n",
        "        offset = (w - h) // 2\n",
        "        image = image[:, offset:offset + h]\n",
        "    elif w < h:\n",
        "        offset = (h - w) // 2\n",
        "        image = image[offset:offset + w]\n",
        "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
        "    return image\n",
        "\n",
        "\n",
        "class NullInversion:\n",
        "\n",
        "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
        "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
        "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
        "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
        "        beta_prod_t = 1 - alpha_prod_t\n",
        "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
        "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
        "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
        "        return prev_sample\n",
        "\n",
        "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
        "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
        "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
        "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
        "        beta_prod_t = 1 - alpha_prod_t\n",
        "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
        "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
        "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
        "        return next_sample\n",
        "\n",
        "    def get_noise_pred_single(self, latents, t, context):\n",
        "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
        "        return noise_pred\n",
        "\n",
        "    def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
        "        latents_input = torch.cat([latents] * 2)\n",
        "        if context is None:\n",
        "            context = self.context\n",
        "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
        "        noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
        "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
        "        if is_forward:\n",
        "            latents = self.next_step(noise_pred, t, latents)\n",
        "        else:\n",
        "            latents = self.prev_step(noise_pred, t, latents)\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def latent2image(self, latents, return_type='np'):\n",
        "        latents = 1 / 0.18215 * latents.detach()\n",
        "        image = self.model.vae.decode(latents)['sample']\n",
        "        if return_type == 'np':\n",
        "            image = (image / 2 + 0.5).clamp(0, 1)\n",
        "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
        "            image = (image * 255).astype(np.uint8)\n",
        "        return image\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def image2latent(self, image):\n",
        "        with torch.no_grad():\n",
        "            if type(image) is Image:\n",
        "                image = np.array(image)\n",
        "            if type(image) is torch.Tensor and image.dim() == 4:\n",
        "                latents = image\n",
        "            else:\n",
        "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
        "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
        "                latents = latents * 0.18215\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def init_prompt(self, prompt: str):\n",
        "        uncond_input = self.model.tokenizer(\n",
        "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
        "        text_input = self.model.tokenizer(\n",
        "            [prompt],\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.model.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
        "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
        "        self.prompt = prompt\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ddim_loop(self, latent):\n",
        "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
        "        all_latent = [latent]\n",
        "        latent = latent.clone().detach()\n",
        "        for i in range(NUM_DDIM_STEPS):\n",
        "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
        "            noise_pred = self.get_noise_pred_single(latent, t, cond_embeddings)\n",
        "            latent = self.next_step(noise_pred, t, latent)\n",
        "            all_latent.append(latent)\n",
        "        return all_latent\n",
        "\n",
        "    @property\n",
        "    def scheduler(self):\n",
        "        return self.model.scheduler\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ddim_inversion(self, image):\n",
        "        latent = self.image2latent(image)\n",
        "        image_rec = self.latent2image(latent)\n",
        "        ddim_latents = self.ddim_loop(latent)\n",
        "        return image_rec, ddim_latents\n",
        "\n",
        "    def null_optimization(self, latents, num_inner_steps, epsilon):\n",
        "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
        "        uncond_embeddings_list = []\n",
        "        latent_cur = latents[-1]\n",
        "        bar = tqdm(total=num_inner_steps * NUM_DDIM_STEPS)\n",
        "        for i in range(NUM_DDIM_STEPS):\n",
        "            uncond_embeddings = uncond_embeddings.clone().detach()\n",
        "            uncond_embeddings.requires_grad = True\n",
        "            optimizer = Adam([uncond_embeddings], lr=1e-2 * (1. - i / 100.))\n",
        "            latent_prev = latents[len(latents) - i - 2]\n",
        "            t = self.model.scheduler.timesteps[i]\n",
        "            with torch.no_grad():\n",
        "                noise_pred_cond = self.get_noise_pred_single(latent_cur, t, cond_embeddings)\n",
        "            for j in range(num_inner_steps):\n",
        "                noise_pred_uncond = self.get_noise_pred_single(latent_cur, t, uncond_embeddings)\n",
        "                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)\n",
        "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur)\n",
        "                loss = nnf.mse_loss(latents_prev_rec, latent_prev)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_item = loss.item()\n",
        "                bar.update()\n",
        "                if loss_item < epsilon + i * 2e-5:\n",
        "                    break\n",
        "            for j in range(j + 1, num_inner_steps):\n",
        "                bar.update()\n",
        "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
        "            with torch.no_grad():\n",
        "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
        "                latent_cur = self.get_noise_pred(latent_cur, t, False, context)\n",
        "        bar.close()\n",
        "        return uncond_embeddings_list\n",
        "\n",
        "    def invert(self, image_path: str, prompt: str, offsets=(0,0,0,0), num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False):\n",
        "        self.init_prompt(prompt)\n",
        "        ptp_utils.register_attention_control(self.model, None)\n",
        "        image_gt = load_512(image_path, *offsets)\n",
        "        if verbose:\n",
        "            print(\"DDIM inversion...\")\n",
        "        image_rec, ddim_latents = self.ddim_inversion(image_gt)\n",
        "        if verbose:\n",
        "            print(\"Null-text optimization...\")\n",
        "        uncond_embeddings = self.null_optimization(ddim_latents, num_inner_steps, early_stop_epsilon)\n",
        "        return (image_gt, image_rec), ddim_latents[-1], uncond_embeddings\n",
        "\n",
        "\n",
        "    def __init__(self, model):\n",
        "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
        "                                  set_alpha_to_one=False)\n",
        "        self.model = model\n",
        "        self.tokenizer = self.model.tokenizer\n",
        "        self.model.scheduler.set_timesteps(NUM_DDIM_STEPS)\n",
        "        self.prompt = None\n",
        "        self.context = None\n",
        "\n",
        "null_inversion = NullInversion(ldm_stable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c919e093-998c-4e4c-92a2-dc9517ef8ea4",
      "metadata": {
        "id": "c919e093-998c-4e4c-92a2-dc9517ef8ea4"
      },
      "source": [
        "## Infernce Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9499145-1a2b-4c91-900e-093c0c08043c",
      "metadata": {
        "id": "f9499145-1a2b-4c91-900e-093c0c08043c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def text2image_ldm_stable(\n",
        "    model,\n",
        "    prompt:  List[str],\n",
        "    controller,\n",
        "    num_inference_steps: int = 50,\n",
        "    guidance_scale: Optional[float] = 7.5,\n",
        "    generator: Optional[torch.Generator] = None,\n",
        "    latent: Optional[torch.FloatTensor] = None,\n",
        "    uncond_embeddings=None,\n",
        "    start_time=50,\n",
        "    return_type='image'\n",
        "):\n",
        "    batch_size = len(prompt)\n",
        "    ptp_utils.register_attention_control(model, controller)\n",
        "    height = width = 512\n",
        "\n",
        "    text_input = model.tokenizer(\n",
        "        prompt,\n",
        "        padding=\"max_length\",\n",
        "        max_length=model.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "    max_length = text_input.input_ids.shape[-1]\n",
        "    if uncond_embeddings is None:\n",
        "        uncond_input = model.tokenizer(\n",
        "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
        "    else:\n",
        "        uncond_embeddings_ = None\n",
        "\n",
        "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
        "    model.scheduler.set_timesteps(num_inference_steps)\n",
        "    for i, t in enumerate(tqdm(model.scheduler.timesteps[-start_time:])):\n",
        "        if uncond_embeddings_ is None:\n",
        "            context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
        "        else:\n",
        "            context = torch.cat([uncond_embeddings_, text_embeddings])\n",
        "        latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False)\n",
        "\n",
        "    if return_type == 'image':\n",
        "        image = ptp_utils.latent2image(model.vae, latents)\n",
        "    else:\n",
        "        image = latents\n",
        "    return image, latent\n",
        "\n",
        "\n",
        "\n",
        "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None, uncond_embeddings=None, verbose=True):\n",
        "    if run_baseline:\n",
        "        print(\"w.o. prompt-to-prompt\")\n",
        "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
        "        print(\"with prompt-to-prompt\")\n",
        "    images, x_t = text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DDIM_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, uncond_embeddings=uncond_embeddings)\n",
        "    if verbose:\n",
        "        ptp_utils.view_images(images)\n",
        "    return images, x_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef5cbc6-2581-415f-b608-67f2e87c32f5",
      "metadata": {
        "id": "6ef5cbc6-2581-415f-b608-67f2e87c32f5"
      },
      "outputs": [],
      "source": [
        "image_path = \"./gnochi_mirror.jpeg\"\n",
        "prompt = \"a cat sitting next to a mirror\"\n",
        "(image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(image_path, prompt, offsets=(0,0,200,0), verbose=True)\n",
        "\n",
        "print(\"Modify or remove offsets according to your image!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36d450d-6764-4195-9ff1-842b2f60249e",
      "metadata": {
        "id": "a36d450d-6764-4195-9ff1-842b2f60249e"
      },
      "outputs": [],
      "source": [
        "prompts = [prompt]\n",
        "controller = AttentionStore()\n",
        "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
        "print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image\")\n",
        "ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
        "show_cross_attention(controller, 16, [\"up\", \"down\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da742d87-b48e-40e1-8b8e-c0f9b7528cc9",
      "metadata": {
        "id": "da742d87-b48e-40e1-8b8e-c0f9b7528cc9"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a cat sitting next to a mirror\",\n",
        "           \"a tiger sitting next to a mirror\"\n",
        "        ]\n",
        "\n",
        "cross_replace_steps = {'default_': .8,}\n",
        "self_replace_steps = .5\n",
        "blend_word = ((('cat',), (\"tiger\",))) # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), (\"cat\",))).\n",
        "eq_params = {\"words\": (\"tiger\",), \"values\": (2,)} # amplify attention to the word \"tiger\" by *2\n",
        "\n",
        "controller = make_controller(prompts, True, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
        "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n",
        "\n",
        "print(\"Image is highly affected by the self_replace_steps, usually 0.4 is a good default value, but you may want to try the range 0.3,0.4,0.5,0.7 \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861db4be-72cc-4d8f-969b-bba1bf45bb50",
      "metadata": {
        "id": "861db4be-72cc-4d8f-969b-bba1bf45bb50"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a cat sitting next to a mirror\",\n",
        "           \"a silver cat sculpture sitting next to a mirror\"\n",
        "        ]\n",
        "\n",
        "cross_replace_steps = {'default_': .8, }\n",
        "self_replace_steps = .6\n",
        "blend_word = ((('cat',), (\"cat\",))) # for local edit\n",
        "eq_params = {\"words\": (\"silver\", 'sculpture', ), \"values\": (2,2,)}  # amplify attention to the words \"silver\" and \"sculpture\" by *2\n",
        "\n",
        "controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
        "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b768dd2-a139-4163-823e-15318441ea49",
      "metadata": {
        "id": "4b768dd2-a139-4163-823e-15318441ea49"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a cat sitting next to a mirror\",\n",
        "           \"watercolor painting of a cat sitting next to a mirror\"\n",
        "        ]\n",
        "\n",
        "cross_replace_steps = {'default_': .8, }\n",
        "self_replace_steps = .7\n",
        "blend_word = None\n",
        "eq_params = {\"words\": (\"watercolor\",  ), \"values\": (5, 2,)}  # amplify attention to the word \"watercolor\" by 5\n",
        "\n",
        "controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
        "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X4Msw5kQqw__",
      "metadata": {
        "id": "X4Msw5kQqw__"
      },
      "source": [
        "# 새로운 실험 1: TI 형태의 최적화"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IR8kV-i3mlc9",
      "metadata": {
        "id": "IR8kV-i3mlc9"
      },
      "source": [
        "1: placeholder 토큰 추가 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QgiWcw6Jmb6t",
      "metadata": {
        "id": "QgiWcw6Jmb6t"
      },
      "outputs": [],
      "source": [
        "# === Textual Inversion: placeholder 토큰 추가 ===\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def add_placeholder_token(pipe,\n",
        "                          placeholder_token: str = \"<sks-cat>\",\n",
        "                          initializer_token: str = \"cat\"):\n",
        "    \"\"\"\n",
        "    SD 파이프라인에 placeholder 토큰을 추가하고,\n",
        "    초기 임베딩을 initializer_token(예: 'cat')에서 복사.\n",
        "    \"\"\"\n",
        "    tokenizer = pipe.tokenizer\n",
        "    text_encoder = pipe.text_encoder\n",
        "\n",
        "    # 1) 토큰 추가\n",
        "    num_added = tokenizer.add_tokens(placeholder_token)\n",
        "    if num_added == 0:\n",
        "        print(f\"[add_placeholder_token] '{placeholder_token}' is already in tokenizer.\")\n",
        "    else:\n",
        "        print(f\"[add_placeholder_token] Added {num_added} token(s): {placeholder_token}\")\n",
        "\n",
        "    # 2) 텍스트 인코더의 임베딩 사이즈를 tokenizer에 맞게 확장\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # 3) placeholder 임베딩을 initializer 임베딩으로 초기화\n",
        "    token_embeds = text_encoder.get_input_embeddings().weight  # (V, d)\n",
        "    placeholder_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "    init_id = tokenizer.convert_tokens_to_ids(initializer_token)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_embeds[placeholder_id] = token_embeds[init_id]\n",
        "\n",
        "    print(f\"[add_placeholder_token] placeholder id = {placeholder_id}, init id = {init_id}\")\n",
        "    return placeholder_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HYEmkjdLmt14",
      "metadata": {
        "id": "HYEmkjdLmt14"
      },
      "source": [
        "2: B 이미지를 latent로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5XcCZCVAmvy4",
      "metadata": {
        "id": "5XcCZCVAmvy4"
      },
      "outputs": [],
      "source": [
        "# === Textual Inversion: 학습용 latent 준비 ===\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def prepare_latents_for_textual_inversion(null_inversion,\n",
        "                                          image_paths,\n",
        "                                          device=device):\n",
        "    \"\"\"\n",
        "    이미지 경로 리스트를 받아서, VAE로 latent로 변환한 뒤 하나의 텐서로 합침.\n",
        "    \"\"\"\n",
        "    latents_list = []\n",
        "    for p in image_paths:\n",
        "        img = load_512(p)  # 이미 NTI 코드에서 정의됨 (512 정사각형 crop + resize)\n",
        "        lat = null_inversion.image2latent(img)  # (1, 4, 64, 64)\n",
        "        latents_list.append(lat)\n",
        "\n",
        "    latents = torch.cat(latents_list, dim=0).to(device)  # (N, 4, 64, 64)\n",
        "    print(f\"[prepare_latents_for_textual_inversion] latents shape: {latents.shape}\")\n",
        "    return latents\n",
        "\n",
        "# 예시: B 정체성 이미지들이 들어있는 폴더 지정\n",
        "b_image_paths = sorted(glob(\"./cat.jpeg\"))  # 혹은 .jpg 등\n",
        "print(\"num B images:\", len(b_image_paths))\n",
        "\n",
        "latents_B = prepare_latents_for_textual_inversion(null_inversion, b_image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BX5Hj7f2lNw-",
      "metadata": {
        "id": "BX5Hj7f2lNw-"
      },
      "source": [
        "3: textual inversion 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adfb554-148b-45af-ae8b-17b213f9070f",
      "metadata": {
        "id": "6adfb554-148b-45af-ae8b-17b213f9070f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ptp_utils import register_attention_control\n",
        "\n",
        "def train_textual_inversion_for_identity(\n",
        "    pipe,\n",
        "    latents_B: torch.Tensor,\n",
        "    placeholder_token: str = \"<sks-cat>\",\n",
        "    prompt_template: str = \"a photo of a {}\",\n",
        "    num_train_steps: int = 800,\n",
        "    batch_size: int = 1,\n",
        "    lr: float = 5e-4,\n",
        "    max_grad_norm: float = 1.0,\n",
        "    device=None,\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 0. TI 동안 P2P 컨트롤 끄기\n",
        "    register_attention_control(pipe, None)\n",
        "\n",
        "    # 1. 디바이스 세팅\n",
        "    pipe.to(device)\n",
        "    pipe.unet.to(device)\n",
        "    pipe.vae.to(device)\n",
        "    pipe.text_encoder.to(device)\n",
        "\n",
        "    tokenizer        = pipe.tokenizer\n",
        "    text_encoder     = pipe.text_encoder\n",
        "    unet             = pipe.unet\n",
        "    noise_scheduler  = pipe.scheduler\n",
        "\n",
        "    # 2. 스케줄러 버퍼를 전부 device 로\n",
        "    def _to_dev(x):\n",
        "        return x.to(device) if isinstance(x, torch.Tensor) else x\n",
        "\n",
        "    if hasattr(noise_scheduler, \"alphas_cumprod\"):\n",
        "        noise_scheduler.alphas_cumprod = _to_dev(noise_scheduler.alphas_cumprod)\n",
        "    if hasattr(noise_scheduler, \"alphas\"):\n",
        "        noise_scheduler.alphas = _to_dev(noise_scheduler.alphas)\n",
        "    if hasattr(noise_scheduler, \"betas\"):\n",
        "        noise_scheduler.betas = _to_dev(noise_scheduler.betas)\n",
        "    if hasattr(noise_scheduler, \"final_alpha_cumprod\"):\n",
        "        noise_scheduler.final_alpha_cumprod = _to_dev(noise_scheduler.final_alpha_cumprod)\n",
        "    if hasattr(noise_scheduler, \"timesteps\") and isinstance(noise_scheduler.timesteps, torch.Tensor):\n",
        "        noise_scheduler.timesteps = noise_scheduler.timesteps.to(device)\n",
        "\n",
        "    # 3. placeholder 토큰 추가\n",
        "    placeholder_id = add_placeholder_token(\n",
        "        pipe,\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=\"cat\",\n",
        "    )\n",
        "    print(f\"[TI] placeholder token '{placeholder_token}' id = {placeholder_id}\")\n",
        "\n",
        "    # 4. UNet / text_encoder freeze, embedding weight만 학습\n",
        "    for p in unet.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    for p in text_encoder.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    token_embeds = text_encoder.get_input_embeddings().weight  # (V, d)\n",
        "    token_embeds.requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.Adam([token_embeds], lr=lr)\n",
        "\n",
        "    latents_B = latents_B.to(device)\n",
        "    N = latents_B.shape[0]\n",
        "    global_step = 0\n",
        "\n",
        "    print(f\"[TI] start training for token {placeholder_token}\")\n",
        "    print(f\"     num_train_steps={num_train_steps}, batch_size={batch_size}, N={N}\")\n",
        "\n",
        "    while global_step < num_train_steps:\n",
        "        # 4-1. 배치\n",
        "        idx = torch.randint(0, N, (batch_size,), device=device)\n",
        "        clean_latents = latents_B[idx]\n",
        "\n",
        "        noise = torch.randn_like(clean_latents)\n",
        "        timesteps = torch.randint(\n",
        "            0,\n",
        "            noise_scheduler.num_train_timesteps,\n",
        "            (batch_size,),\n",
        "            device=device,\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        noisy_latents = noise_scheduler.add_noise(clean_latents, noise, timesteps)\n",
        "\n",
        "        # 4-2. 텍스트 인코딩\n",
        "        prompts = [prompt_template.format(placeholder_token)] * batch_size\n",
        "        text_inputs = tokenizer(\n",
        "            prompts,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        encoder_hidden_states = text_encoder(text_inputs.input_ids)[0]\n",
        "\n",
        "        # 4-3. UNet 예측 & loss\n",
        "        model_pred = unet(\n",
        "            noisy_latents,\n",
        "            timesteps,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "        ).sample\n",
        "\n",
        "        # 구버전 스케줄러: epsilon 예측 가정\n",
        "        target = noise\n",
        "\n",
        "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "        # 4-4. 역전파 & placeholder 행만 gradient 유지\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if max_grad_norm is not None:\n",
        "            torch.nn.utils.clip_grad_norm_([token_embeds], max_grad_norm)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if token_embeds.grad is not None:\n",
        "                grad = token_embeds.grad\n",
        "                mask = torch.zeros_like(grad)\n",
        "                mask[placeholder_id] = grad[placeholder_id]\n",
        "                grad.copy_(mask)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"[TI] step {global_step:04d}/{num_train_steps} | loss = {loss.item():.6f}\")\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "    print(f\"[TI] finished. token '{placeholder_token}' embedding updated.\")\n",
        "    return placeholder_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHk94jK1qNu-",
      "metadata": {
        "id": "oHk94jK1qNu-"
      },
      "source": [
        "실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CEPBE1aGWpuO",
      "metadata": {
        "id": "CEPBE1aGWpuO"
      },
      "outputs": [],
      "source": [
        "# --- P2P AttentionStore: between_steps를 항상 no_grad에서 실행하도록 패치 ---\n",
        "\n",
        "import torch\n",
        "import ptp_utils\n",
        "import types\n",
        "\n",
        "def patch_between_steps_no_grad():\n",
        "    patched = False\n",
        "    for name, obj in vars(ptp_utils).items():\n",
        "        # 클래스이면서 between_steps 메서드를 가진 애들만 골라서 패치\n",
        "        if isinstance(obj, type) and hasattr(obj, \"between_steps\"):\n",
        "            orig_between = obj.between_steps\n",
        "\n",
        "            # 이미 패치된 것 같으면 건너뜀 (id 체크)\n",
        "            if getattr(orig_between, \"_patched_no_grad\", False):\n",
        "                continue\n",
        "\n",
        "            def make_patched(orig):\n",
        "                def patched_between(self, *args, **kwargs):\n",
        "                    # <-- 여기서 grad 비활성화\n",
        "                    with torch.no_grad():\n",
        "                        return orig(self, *args, **kwargs)\n",
        "                # 나중에 중복 패치 방지용 플래그\n",
        "                patched_between._patched_no_grad = True\n",
        "                return patched_between\n",
        "\n",
        "            obj.between_steps = make_patched(orig_between)\n",
        "            print(f\"[patch_between_steps_no_grad] Patched {name}.between_steps\")\n",
        "            patched = True\n",
        "\n",
        "    if not patched:\n",
        "        print(\"[patch_between_steps_no_grad] No class with between_steps found in ptp_utils.\")\n",
        "\n",
        "patch_between_steps_no_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hSx0MhW4qMv8",
      "metadata": {
        "id": "hSx0MhW4qMv8"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# A(배경) + B(정체성) 이미지 합성 전체 파이프라인\n",
        "#   - 1) B로 Textual Inversion\n",
        "#   - 2) A에 Null-text Inversion\n",
        "#   - 3) P2P로 \"cat\" -> placeholder_token 치환 (B 정체성 삽입)\n",
        "# ================================\n",
        "import torch\n",
        "from glob import glob\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 0. 유저 설정: A/B 경로, 프롬프트, 토큰 이름\n",
        "# --------------------------------------------------\n",
        "# (1) A: 배경 이미지 경로 + 프롬프트 (cat 이 들어간 문장)\n",
        "image_A_path = \"./gnochi_mirror.jpeg\"   # <- 너의 실제 A 이미지 경로로 변경\n",
        "prompt_A     = \"a cat sitting next to a mirror\"\n",
        "\n",
        "# (2) B: 정체성 이미지들이 들어있는 폴더 (여러 장 추천)\n",
        "b_image_glob = \"./cat.jpeg\"   # *.jpg 등으로 바꿔도 됨\n",
        "\n",
        "# (3) Textual Inversion용 placeholder 토큰\n",
        "placeholder_token = \"<sks-cat>\"   # 프롬프트 안에서 그대로 사용될 문자열\n",
        "initializer_token = \"cat\"         # 처음엔 cat 임베딩에서 시작\n",
        "\n",
        "# (4) cat이라는 단어를 어떤 토큰으로 취급할지\n",
        "source_word = \"cat\"               # A 프롬프트 안에서 교체하고 싶은 단어\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. B 이미지들로 Textual Inversion 수행\n",
        "#    (이미 학습해둔 토큰이 있으면, 이 블록은 주석 처리해도 됨)\n",
        "# --------------------------------------------------\n",
        "b_image_paths = sorted(glob(b_image_glob))\n",
        "print(f\"#B images = {len(b_image_paths)}\")\n",
        "if len(b_image_paths) == 0:\n",
        "    raise ValueError(\"B 정체성 이미지가 없습니다. b_image_glob 경로를 확인하세요.\")\n",
        "\n",
        "# B 이미지를 latent로 변환\n",
        "latents_B = prepare_latents_for_textual_inversion(\n",
        "    null_inversion,   # 같은 VAE를 쓰기 위해 null_inversion 객체 재사용\n",
        "    b_image_paths,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Textual Inversion 학습\n",
        "_ = train_textual_inversion_for_identity(\n",
        "    pipe=ldm_stable,\n",
        "    latents_B=latents_B,\n",
        "    placeholder_token=placeholder_token,\n",
        "    prompt_template=\"a photo of a {}\",   # -> \"a photo of a <sks-cat>\"\n",
        "    num_train_steps=800,                 # 필요하면 1000 이상으로 늘려도 됨\n",
        "    batch_size=1,\n",
        "    lr=5e-4,\n",
        "    max_grad_norm=1.0,\n",
        "    device=device,\n",
        ")\n",
        "# 이 시점에서 placeholder_token 임베딩은 B 정체성을 담고 있음.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. A 이미지에 대해 Null-text Inversion 수행\n",
        "#    (프롬프트는 원래 A를 잘 설명하는 문장: 여기선 cat 그대로 사용)\n",
        "# --------------------------------------------------\n",
        "(image_gt, image_rec), x_t, uncond_embeddings = null_inversion.invert(\n",
        "    image_A_path,\n",
        "    prompt_A,\n",
        ")  # offsets, num_inner_steps, epsilon 등은 기본값 사용\n",
        "\n",
        "print(\"Inversion done.\")\n",
        "print(\"  x_t shape:\", x_t.shape)\n",
        "print(\"  len(uncond_embeddings):\", len(uncond_embeddings))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. P2P 편집: source 프롬프트 vs target 프롬프트\n",
        "#    - source:  A (원본)\n",
        "#    - target:  A에서 'cat'만 placeholder_token으로 치환 → B 정체성 삽입\n",
        "# --------------------------------------------------\n",
        "# target 프롬프트: 간단히 문자열 치환으로 만들자\n",
        "prompt_B = prompt_A.replace(source_word, placeholder_token)\n",
        "\n",
        "prompts = [prompt_A, prompt_B]\n",
        "print(\"Source prompt :\", prompts[0])\n",
        "print(\"Target prompt :\", prompts[1])\n",
        "\n",
        "# P2P 세부 설정 (기존 노트북 예제 값에서 크게 벗어나지 않게 설정)\n",
        "cross_replace_steps = {\"default_\": 0.8}\n",
        "self_replace_steps  = 0.5\n",
        "\n",
        "# 'cat' 이 있는 영역만, target 쪽의 placeholder_token으로 로컬 편집\n",
        "blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "# equalizer를 쓰고 싶으면 여기서 정의, 지금은 안 씀\n",
        "eq_params = None\n",
        "\n",
        "controller = make_controller(\n",
        "    prompts,\n",
        "    True,                  # is_replace_controller → AttentionReplace 사용\n",
        "    cross_replace_steps,\n",
        "    self_replace_steps,\n",
        "    blend_word,\n",
        "    eq_params,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. NTI 결과(x_t, uncond_embeddings)를 사용해서 편집 샘플링\n",
        "#    - text2image_ldm_stable 안에서, target 프롬프트의\n",
        "#      placeholder_token 임베딩이 곧 B 정체성 임베딩으로 사용됨.\n",
        "# --------------------------------------------------\n",
        "images_edit, _ = text2image_ldm_stable(\n",
        "    ldm_stable,\n",
        "    prompts,\n",
        "    controller,\n",
        "    num_inference_steps=NUM_DDIM_STEPS,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    generator=None,\n",
        "    latent=x_t,                     # NTI에서 얻은 마지막 latent\n",
        "    uncond_embeddings=uncond_embeddings,  # NTI에서 최적화된 uncond 리스트\n",
        "    start_time=NUM_DDIM_STEPS,\n",
        "    return_type=\"image\",\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. 결과 확인 (좌: GT, 중: 단순 재구성, 우: B 정체성으로 합성된 결과)\n",
        "# --------------------------------------------------\n",
        "ptp_utils.view_images([image_gt, image_rec, images_edit[0]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YntP02n_ljAi",
      "metadata": {
        "id": "YntP02n_ljAi"
      },
      "source": [
        "# 실험 2: masking 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aRB4XY7w5IWg",
      "metadata": {
        "id": "aRB4XY7w5IWg"
      },
      "source": [
        "디퓨저 사용으로 수정..?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3rTl3VP65H8R",
      "metadata": {
        "id": "3rTl3VP65H8R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "# [핵심] 구버전 diffusers에서 Attention 모듈 클래스 직접 가져오기\n",
        "from diffusers.models.attention import CrossAttention\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. [구버전 호환] 순정 상태 복구 기능을 포함한 마스크 생성 함수\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_automask_from_attention(model, tokenizer, latents, prompt, target_word_index, resolution=16):\n",
        "\n",
        "    # 1. P2P Controller 등록 (이때 U-Net의 forward 메서드가 P2P용으로 바뀜)\n",
        "    controller = AttentionStore()\n",
        "    register_attention_control(model, controller)\n",
        "\n",
        "    # 2. 마스크 생성 (Forward Pass)\n",
        "    t = torch.tensor([400]).to(model.device)\n",
        "    noisy_latents = model.scheduler.add_noise(latents, torch.randn_like(latents), t)\n",
        "\n",
        "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.unet(noisy_latents, t, encoder_hidden_states=text_embeddings)\n",
        "\n",
        "    # 3. Attention Map 추출\n",
        "    attention_maps = controller.get_average_attention()\n",
        "    target_map = None\n",
        "\n",
        "    for key in attention_maps:\n",
        "        if key == resolution:\n",
        "            attn = attention_maps[key]\n",
        "            target_map = attn[0].mean(0)[:, target_word_index]\n",
        "            break\n",
        "\n",
        "    if target_map is None:\n",
        "        target_map = list(attention_maps.values())[0][0].mean(0)[:, target_word_index]\n",
        "\n",
        "    res_int = int(np.sqrt(target_map.shape[0]))\n",
        "    target_map = target_map.view(1, 1, res_int, res_int)\n",
        "    mask = F.interpolate(target_map, size=(64, 64), mode='bilinear')\n",
        "    mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "    mask[mask < 0.3] = 0   # 0.3 미만은 아예 0으로 날려버림 (Hard Threshold)\n",
        "\n",
        "    controller.reset()\n",
        "\n",
        "    # [핵심 해결책] 4. U-Net을 '순정 상태'로 원상복구 (Monkey Patching Undo)\n",
        "    # P2P가 forward 메서드를 바꿔치기했으므로, 클래스 원본(CrossAttention)의 메서드를 다시 주입합니다.\n",
        "    # 이렇게 하면 P2P의 흔적이 사라지고, 학습 시 Gradient 끊김 문제가 해결됩니다.\n",
        "\n",
        "    print(\"  Restoring U-Net to original state for training...\")\n",
        "    for name, module in model.unet.named_modules():\n",
        "        # 모듈이 CrossAttention 클래스인지 확인\n",
        "        if isinstance(module, CrossAttention):\n",
        "            # 인스턴스 메서드를 클래스 원본 메서드로 덮어씌움\n",
        "            module.forward = CrossAttention.forward.__get__(module, CrossAttention)\n",
        "\n",
        "    print(\"  -> Done.\")\n",
        "\n",
        "    return mask.detach()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. [유지] 최적화 함수 (이전과 동일)\n",
        "# ------------------------------------------------------------------------------\n",
        "def optimize_embedding_with_automask(\n",
        "    model, tokenizer, latents_B,\n",
        "    placeholder_token,\n",
        "    initializer_token,\n",
        "    prompt_template=\"a photo of a {}\",\n",
        "    target_word_index_in_template=4,\n",
        "    num_train_steps=300\n",
        "):\n",
        "    print(f\"Initializing '{placeholder_token}' with '{initializer_token}'...\")\n",
        "\n",
        "    # 1. 토큰 추가\n",
        "    num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
        "    placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "\n",
        "    # 2. 임베딩 리사이즈 및 초기화\n",
        "    model.text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    token_embeds = model.text_encoder.get_input_embeddings().weight.data\n",
        "    init_token_id = tokenizer.encode(initializer_token, add_special_tokens=False)[0]\n",
        "    token_embeds[placeholder_token_id] = token_embeds[init_token_id]\n",
        "\n",
        "    # [디버깅] 초기 임베딩 값 저장 (비교용)\n",
        "    initial_embedding = token_embeds[placeholder_token_id].clone()\n",
        "\n",
        "    # 3. 마스크 생성 (Threshold 0.3 적용 포함)\n",
        "    print(\"Generating Auto-Mask...\")\n",
        "    prompt_for_mask = prompt_template.format(initializer_token)\n",
        "\n",
        "    mask = get_automask_from_attention(\n",
        "        model, tokenizer, latents_B,\n",
        "        prompt=prompt_for_mask,\n",
        "        target_word_index=target_word_index_in_template,\n",
        "        resolution=16\n",
        "    ).detach().clone()\n",
        "\n",
        "    # [수정] Threshold 적용 (User 요청 1번)\n",
        "    mask[mask < 0.3] = 0\n",
        "\n",
        "    # 4. 학습 모드 강제 설정 (매우 중요)\n",
        "    model.text_encoder.train()\n",
        "    model.text_encoder.get_input_embeddings().weight.requires_grad_(True)\n",
        "\n",
        "    # Optimizer 설정\n",
        "    optimizer = torch.optim.Adam([model.text_encoder.get_input_embeddings().weight], lr=1e-3)\n",
        "\n",
        "    train_prompt = prompt_template.format(placeholder_token)\n",
        "    text_input = tokenizer(train_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = text_input.input_ids.to(model.device)\n",
        "\n",
        "    print(f\"Start Optimization (Steps: {num_train_steps})...\")\n",
        "    latents_B = latents_B.detach()\n",
        "\n",
        "    for step in range(num_train_steps):\n",
        "        # ... (노이즈 추가 및 Forward는 기존과 동일) ...\n",
        "        noise = torch.randn_like(latents_B)\n",
        "        bsz = latents_B.shape[0]\n",
        "        timesteps = torch.randint(0, model.scheduler.num_train_timesteps, (bsz,), device=latents_B.device).long()\n",
        "        noisy_latents = model.scheduler.add_noise(latents_B, noise, timesteps)\n",
        "\n",
        "        encoder_hidden_states = model.text_encoder(input_ids)[0]\n",
        "        noise_pred = model.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "        loss_pixel = F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
        "        loss = (loss_pixel * mask).mean()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # [핵심 수정] Gradient 수동 조작 로직 점검\n",
        "        grads = model.text_encoder.get_input_embeddings().weight.grad\n",
        "        if grads is not None:\n",
        "            # 타겟 토큰의 grad만 백업\n",
        "            target_grad = grads[placeholder_token_id, :].clone()\n",
        "            # 전체 grad 0으로 초기화 (다른 토큰 보호)\n",
        "            grads.data.zero_()\n",
        "            # 타겟 토큰 grad 복구\n",
        "            grads.data[placeholder_token_id, :] = target_grad\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"  Step {step}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    # [디버깅] 학습 후 변화량 체크\n",
        "    final_embedding = model.text_encoder.get_input_embeddings().weight.data[placeholder_token_id]\n",
        "    delta = (final_embedding - initial_embedding).norm().item()\n",
        "    print(f\"Optimization Done. Embedding Change (L2 Norm): {delta:.6f}\")\n",
        "\n",
        "    if delta < 1e-4:\n",
        "        print(\"WARNING: 임베딩이 거의 변하지 않았습니다! 학습률(LR)을 높이거나 코드를 점검하세요.\")\n",
        "    else:\n",
        "        print(\"SUCCESS: 임베딩이 유의미하게 업데이트되었습니다.\")\n",
        "\n",
        "    return placeholder_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UJG-Rd9P0t2S",
      "metadata": {
        "id": "UJG-Rd9P0t2S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from glob import glob\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 0. 유저 설정\n",
        "# --------------------------------------------------\n",
        "image_A_path = \"./gnochi_mirror.jpeg\"\n",
        "prompt_A     = \"a cat sitting next to a mirror\"\n",
        "b_image_glob = \"./cat.jpeg\"\n",
        "\n",
        "placeholder_token = \"<sks-cat>\"\n",
        "initializer_token = \"cat\"\n",
        "source_word = \"cat\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. B 이미지들로 Textual Inversion 수행 (Masked Version)\n",
        "# --------------------------------------------------\n",
        "b_image_paths = sorted(glob(b_image_glob))\n",
        "print(f\"#B images = {len(b_image_paths)}\")\n",
        "\n",
        "if len(b_image_paths) == 0:\n",
        "    print(\"Warning: B 이미지가 없습니다.\")\n",
        "else:\n",
        "    # [수정 1] 이미지를 하나만 사용하도록 강제 (Batch Error 방지)\n",
        "    # 여러 장이 있어도 첫 번째 장만 사용하여 Identity를 추출합니다.\n",
        "    b_image_paths = b_image_paths[:1]\n",
        "    print(f\"Using single image for optimization: {b_image_paths}\")\n",
        "\n",
        "    # [수정 2] 기존 함수 사용 (null_inversion 인자 전달)\n",
        "    # 단, 반환값이 Tuple(images, latents)일 수 있으므로 안전하게 처리\n",
        "    raw_output = prepare_latents_for_textual_inversion(\n",
        "        null_inversion,\n",
        "        b_image_paths,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # 튜플인지 확인해서 Latent만 꺼내기\n",
        "    if isinstance(raw_output, tuple) or isinstance(raw_output, list):\n",
        "        latents_B = raw_output[1] # 보통 두 번째 요소가 latent\n",
        "    else:\n",
        "        latents_B = raw_output # 텐서 그대로\n",
        "\n",
        "    latents_B = latents_B.to(ldm_stable.device)\n",
        "    print(f\"Latents loaded. Shape: {latents_B.shape}\") # [1, 4, 64, 64] 확인\n",
        "\n",
        "    # [수정 3] 기존 train_textual_inversion_for_identity 대신\n",
        "    #          위에서 정의한 'Masked Optimization' 함수 호출\n",
        "    optimized_token = optimize_embedding_with_automask(\n",
        "        model=ldm_stable,  # ldm_stable 전달\n",
        "        tokenizer=tokenizer,\n",
        "        latents_B=latents_B,\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=initializer_token,\n",
        "        prompt_template=\"a photo of a {}\",\n",
        "        target_word_index_in_template=4,\n",
        "        num_train_steps=500, # 스텝 수 조절 가능\n",
        "    )\n",
        "\n",
        "    print(f\"Optimization Finished. Token: {optimized_token}\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. A 이미지에 대해 Null-text Inversion 수행 (기존 유지)\n",
        "# --------------------------------------------------\n",
        "(image_gt, image_rec), x_t, uncond_embeddings = null_inversion.invert(\n",
        "    image_A_path,\n",
        "    prompt_A,\n",
        ")\n",
        "print(\"Inversion done.\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. P2P 편집 설정 (기존 유지)\n",
        "# --------------------------------------------------\n",
        "prompt_B = prompt_A.replace(source_word, placeholder_token)\n",
        "prompts = [prompt_A, prompt_B]\n",
        "\n",
        "print(\"Source prompt :\", prompts[0])\n",
        "print(\"Target prompt :\", prompts[1])\n",
        "\n",
        "cross_replace_steps = {\"default_\": 0.8}\n",
        "self_replace_steps  = 0.6 # 너무 낮으면 구조가 깨짐, 적당히 조절\n",
        "\n",
        "blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "controller = make_controller(\n",
        "    prompts,\n",
        "    True,   # AttentionReplace\n",
        "    cross_replace_steps,\n",
        "    self_replace_steps,\n",
        "    blend_word,\n",
        "    None,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. 이미지 생성 (기존 유지)\n",
        "# --------------------------------------------------\n",
        "images_edit, _ = text2image_ldm_stable(\n",
        "    ldm_stable,\n",
        "    prompts,\n",
        "    controller,\n",
        "    num_inference_steps=NUM_DDIM_STEPS,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    generator=None,\n",
        "    latent=x_t,\n",
        "    uncond_embeddings=uncond_embeddings,\n",
        "    start_time=NUM_DDIM_STEPS,\n",
        "    return_type=\"image\",\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. 결과 확인\n",
        "# --------------------------------------------------\n",
        "ptp_utils.view_images([image_gt, image_rec, images_edit[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oqdOkJ7bDiM8",
      "metadata": {
        "id": "oqdOkJ7bDiM8"
      },
      "source": [
        "어째서인지 masking 없을때와의 차이가 없음..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DNQaEsOw8ztl",
      "metadata": {
        "id": "DNQaEsOw8ztl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# [검증 실험] 학습된 Embedding이 B 이미지를 얼마나 잘 재현하는가?\n",
        "# ======================================================\n",
        "\n",
        "# 1. 검증용 프롬프트\n",
        "test_prompt = f\"a photo of {placeholder_token}\"  # \"a photo of <sks-cat>\"\n",
        "print(f\"Testing Prompt: {test_prompt}\")\n",
        "\n",
        "# 2. Reference B 이미지 준비 (시각화용)\n",
        "# b_image_paths[0]가 유효한지 확인\n",
        "if len(b_image_paths) > 0:\n",
        "    img_b_origin = Image.open(b_image_paths[0]).convert(\"RGB\").resize((512, 512))\n",
        "else:\n",
        "    print(\"Warning: B 이미지가 없습니다.\")\n",
        "    img_b_origin = Image.new(\"RGB\", (512, 512), (255, 255, 255)) # 빈 이미지\n",
        "\n",
        "# 3. P2P 컨트롤러 없이 순수 생성 (일반적인 Text-to-Image)\n",
        "ver_images = []\n",
        "print(\"Generating verification images...\")\n",
        "\n",
        "# 랜덤 시드 4개를 돌려봅니다.\n",
        "for i in range(4):\n",
        "    g_cpu = torch.Generator().manual_seed(8888 + i)\n",
        "\n",
        "    # 노트북 내에 정의된 AttentionStore 사용 (ptp_utils 아님)\n",
        "    dummy_controller = AttentionStore()\n",
        "\n",
        "    # 생성\n",
        "    image, _ = text2image_ldm_stable(\n",
        "        ldm_stable,\n",
        "        [test_prompt],\n",
        "        dummy_controller,\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "        generator=g_cpu,\n",
        "        latent=None, # 랜덤 노이즈에서 시작 (Reconstruction 능력 검증)\n",
        "        uncond_embeddings=None,\n",
        "        start_time=50,\n",
        "        return_type=\"image\"\n",
        "    )\n",
        "\n",
        "    # image[0]는 (512, 512, 3) 형태의 numpy array라고 가정\n",
        "    ver_images.append(image[0])\n",
        "\n",
        "    # 메모리 정리\n",
        "    dummy_controller.reset()\n",
        "\n",
        "# 4. 결과 시각화 (Matplotlib 사용 - 에러 없음)\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# (1) Reference Image B\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.imshow(img_b_origin)\n",
        "plt.title(\"Reference (Input)\", fontsize=12)\n",
        "plt.axis('off')\n",
        "\n",
        "# (2) Generated Images\n",
        "for i, img in enumerate(ver_images):\n",
        "    plt.subplot(1, 5, i + 2)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Gen {i+1} (<sks-cat>)\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HlN5zsPmGb1z",
      "metadata": {
        "id": "HlN5zsPmGb1z"
      },
      "source": [
        "mask 부터 다시 잘 찾아보자.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gBwELwfzGdlg",
      "metadata": {
        "id": "gBwELwfzGdlg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from diffusers.models.attention import CrossAttention\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 시각화 전용 함수: Raw Attention Map을 뽑아냅니다.\n",
        "# ------------------------------------------------------------------------------\n",
        "def visualize_attention_thresholds(model, tokenizer, image_path, prompt, target_word_index, thresholds=[0.2, 0.4, 0.6, 0.8]):\n",
        "\n",
        "    # 1. 이미지 로드 및 전처리\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\").resize((512, 512))\n",
        "    image_np = np.array(image_pil).astype(np.float32) / 127.5 - 1.0\n",
        "    image_tensor = torch.from_numpy(image_np.transpose(2, 0, 1)).unsqueeze(0).to(model.device)\n",
        "\n",
        "    # Latent 인코딩\n",
        "    with torch.no_grad():\n",
        "        latents = model.vae.encode(image_tensor).latent_dist.sample() * 0.18215\n",
        "\n",
        "    # 2. Attention Store 등록\n",
        "    controller = AttentionStore()\n",
        "    register_attention_control(model, controller)\n",
        "\n",
        "    # 3. Forward Pass (노이즈 추가하여 구조 파악)\n",
        "    t = torch.tensor([400]).to(model.device)\n",
        "    noisy_latents = model.scheduler.add_noise(latents, torch.randn_like(latents), t)\n",
        "\n",
        "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.unet(noisy_latents, t, encoder_hidden_states=text_embeddings)\n",
        "\n",
        "    # 4. Attention Map 추출 (16x16 해상도 기준)\n",
        "    attention_maps = controller.get_average_attention()\n",
        "    target_map = None\n",
        "    resolution = 16\n",
        "\n",
        "    for key in attention_maps:\n",
        "        if key == resolution:\n",
        "            attn = attention_maps[key]\n",
        "            target_map = attn[0].mean(0)[:, target_word_index]\n",
        "            break\n",
        "\n",
        "    if target_map is None: # fallback\n",
        "        target_map = list(attention_maps.values())[0][0].mean(0)[:, target_word_index]\n",
        "\n",
        "    # 5. 시각화를 위해 512x512로 Upscaling\n",
        "    res_int = int(np.sqrt(target_map.shape[0]))\n",
        "    target_map = target_map.view(1, 1, res_int, res_int)\n",
        "\n",
        "    # Raw Map (0~1 정규화 전, 분포 확인용)\n",
        "    raw_map_flat = target_map.flatten()\n",
        "    print(f\"Attention Value Stats | Min: {raw_map_flat.min():.4f}, Max: {raw_map_flat.max():.4f}, Mean: {raw_map_flat.mean():.4f}\")\n",
        "\n",
        "    # 시각화용 맵 (0~1 정규화)\n",
        "    attn_map_highres = F.interpolate(target_map, size=(512, 512), mode='bilinear')\n",
        "    attn_map_norm = (attn_map_highres - attn_map_highres.min()) / (attn_map_highres.max() - attn_map_highres.min())\n",
        "    attn_map_norm = attn_map_norm.squeeze().cpu().numpy()\n",
        "\n",
        "    # 6. Cleanup (모델 원상복구)\n",
        "    controller.reset()\n",
        "    for name, module in model.unet.named_modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = CrossAttention.forward.__get__(module, CrossAttention)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 7. 시각화 (Matplotlib)\n",
        "    # --------------------------------------------------------------------------\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # (1) 원본 이미지\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(image_pil)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # (2) Raw Attention Heatmap\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.imshow(attn_map_norm, cmap='jet')\n",
        "    plt.title(\"Raw Attention Map ('cat')\")\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "\n",
        "    # (3) Superimposed (이미지 위에 맵 겹치기)\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.imshow(image_pil)\n",
        "    plt.imshow(attn_map_norm, cmap='jet', alpha=0.5) # 반투명 오버레이\n",
        "    plt.title(\"Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # (4) Threshold 적용 결과들\n",
        "    for i, thresh in enumerate(thresholds):\n",
        "        if i >= 3: break # 공간상 3개만 보여줌\n",
        "        mask_binary = (attn_map_norm > thresh).astype(np.float32)\n",
        "\n",
        "        # 마스크가 적용된 이미지 (배경을 검게 처리)\n",
        "        masked_img = np.array(image_pil).astype(np.float32) / 255.0\n",
        "        masked_img = masked_img * mask_binary[:, :, None] # Broadcast\n",
        "\n",
        "        plt.subplot(2, 3, 4 + i)\n",
        "        plt.imshow(masked_img)\n",
        "        plt.title(f\"Threshold > {thresh}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 실행: B 이미지에 대해 'cat'이 어디 잡히는지 확인\n",
        "# ==============================================================================\n",
        "target_image_path = b_image_paths[0] # B 이미지 경로\n",
        "check_prompt = \"a photo of a cat\"    # B 이미지를 설명하는 일반 프롬프트\n",
        "\n",
        "visualize_attention_thresholds(\n",
        "    model=ldm_stable,\n",
        "    tokenizer=tokenizer,\n",
        "    image_path=target_image_path,\n",
        "    prompt=check_prompt,\n",
        "    target_word_index=4,  # \"a photo of a cat\" -> cat index is 4\n",
        "    thresholds=[0.3, 0.5, 0.7] # 보고 싶은 기준치 설정\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v0yR_tdB4wKm",
      "metadata": {
        "id": "v0yR_tdB4wKm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from glob import glob\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 0. 유저 설정\n",
        "# --------------------------------------------------\n",
        "image_A_path = \"./gnochi_mirror.jpeg\"\n",
        "prompt_A     = \"a cat sitting next to a mirror\"\n",
        "b_image_glob = \"./cat.jpeg\"  # (주의) 대비가 확실한 새로운 고양이 사진 권장\n",
        "\n",
        "placeholder_token = \"<sks-cat>\"\n",
        "initializer_token = \"cat\"\n",
        "source_word = \"cat\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. B 이미지들로 Textual Inversion 수행 (Masked Version)\n",
        "# --------------------------------------------------\n",
        "b_image_paths = sorted(glob(b_image_glob))\n",
        "print(f\"#B images = {len(b_image_paths)}\")\n",
        "\n",
        "if len(b_image_paths) == 0:\n",
        "    print(\"Warning: B 이미지가 없습니다.\")\n",
        "else:\n",
        "    # 1장만 사용 강제\n",
        "    b_image_paths = b_image_paths[:1]\n",
        "    print(f\"Using single image for optimization: {b_image_paths}\")\n",
        "\n",
        "    # Latent 추출\n",
        "    raw_output = prepare_latents_for_textual_inversion(\n",
        "        null_inversion,\n",
        "        b_image_paths,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    if isinstance(raw_output, tuple) or isinstance(raw_output, list):\n",
        "        latents_B = raw_output[1]\n",
        "    else:\n",
        "        latents_B = raw_output\n",
        "\n",
        "    latents_B = latents_B.to(ldm_stable.device)\n",
        "    print(f\"Latents loaded. Shape: {latents_B.shape}\")\n",
        "\n",
        "    # Masked Optimization 실행\n",
        "    # (주의: optimize_embedding_with_automask 함수가 정의되어 있어야 함)\n",
        "    optimized_token = optimize_embedding_with_automask(\n",
        "        model=ldm_stable,\n",
        "        tokenizer=tokenizer,\n",
        "        latents_B=latents_B,\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=initializer_token,\n",
        "        prompt_template=\"a photo of a {}\",\n",
        "        target_word_index_in_template=4,\n",
        "        num_train_steps=500,\n",
        "    )\n",
        "\n",
        "    print(f\"Optimization Finished. Token: {optimized_token}\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. A 이미지에 대해 Null-text Inversion 수행\n",
        "# --------------------------------------------------\n",
        "(image_gt, image_rec), x_t, uncond_embeddings = null_inversion.invert(\n",
        "    image_A_path,\n",
        "    prompt_A,\n",
        ")\n",
        "print(\"Inversion done.\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. P2P 편집 설정 (핵심 수정 파트!)\n",
        "# --------------------------------------------------\n",
        "prompt_B = prompt_A.replace(source_word, placeholder_token)\n",
        "prompts = [prompt_A, prompt_B]\n",
        "\n",
        "print(\"Source prompt :\", prompts[0])\n",
        "print(\"Target prompt :\", prompts[1])\n",
        "\n",
        "# [수정됨] 구조 강제 비율을 0.8 -> 0.4로 대폭 낮춤\n",
        "# 0.4: 초반 40%만 원본 구조를 따르고, 나머지 60%는 새로운 임베딩이 자유롭게 그림\n",
        "cross_replace_steps = {\"default_\": 0.4}\n",
        "self_replace_steps  = 0.4\n",
        "\n",
        "blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "controller = make_controller(\n",
        "    prompts,\n",
        "    True,   # AttentionReplace\n",
        "    cross_replace_steps,\n",
        "    self_replace_steps,\n",
        "    blend_word,\n",
        "    None,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. 이미지 생성\n",
        "# --------------------------------------------------\n",
        "# [Tip] 만약 결과가 여전히 안 변한다면, 아래 uncond_embeddings=uncond_embeddings를\n",
        "#       uncond_embeddings=None 으로 바꿔서 NTI를 꺼보세요.\n",
        "images_edit, _ = text2image_ldm_stable(\n",
        "    ldm_stable,\n",
        "    prompts,\n",
        "    controller,\n",
        "    num_inference_steps=NUM_DDIM_STEPS,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    generator=None,\n",
        "    latent=x_t,\n",
        "    uncond_embeddings=None, # NTI 적용 (너무 강하면 None으로 변경 시도)\n",
        "    start_time=NUM_DDIM_STEPS,\n",
        "    return_type=\"image\",\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. 결과 확인\n",
        "# --------------------------------------------------\n",
        "ptp_utils.view_images([image_gt, image_rec, images_edit[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YDprM3Lz8Nd-",
      "metadata": {
        "id": "YDprM3Lz8Nd-"
      },
      "source": [
        "검정 고양이 어디감.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlDwGluW73Gn",
      "metadata": {
        "id": "hlDwGluW73Gn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# [New] Slerp 함수 정의: 노이즈의 분산(에너지)을 유지하며 섞어줍니다.\n",
        "# ------------------------------------------------------------------------------\n",
        "def slerp(val, low, high):\n",
        "    \"\"\"\n",
        "    Spherical Linear Interpolation\n",
        "    val: interpolation factor (0.0 = low, 1.0 = high)\n",
        "    low: starting vector\n",
        "    high: target vector\n",
        "    \"\"\"\n",
        "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
        "\n",
        "    # 두 벡터 사이의 각도 계산 (Clamp for numerical stability)\n",
        "    dot = (low_norm * high_norm).sum(1)\n",
        "    dot = torch.clamp(dot, -1, 1)\n",
        "    omega = torch.acos(dot).unsqueeze(1)\n",
        "\n",
        "    so = torch.sin(omega)\n",
        "    # 0에 가까운 경우(두 벡터가 평행) 예외 처리\n",
        "    if so.abs().sum() < 1e-6:\n",
        "        return (1.0 - val) * low + val * high\n",
        "\n",
        "    res = (torch.sin((1.0 - val) * omega) / so) * low + (torch.sin(val * omega) / so) * high\n",
        "    return res\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. 안전 장치: x_t 데이터 타입 및 장치 보정\n",
        "# ------------------------------------------------------------------------------\n",
        "if 'x_t' not in locals() or x_t is None:\n",
        "    print(\"Error: 'x_t' 변수가 없습니다. 위쪽의 [2. Null-text Inversion] 셀을 먼저 실행해주세요!\")\n",
        "else:\n",
        "    print(\"x_t found. Checking type...\")\n",
        "\n",
        "    # x_t가 리스트라면 첫 번째 요소를 꺼냅니다.\n",
        "    if isinstance(x_t, list):\n",
        "        start_noise = x_t[0]\n",
        "    else:\n",
        "        start_noise = x_t\n",
        "\n",
        "    # 모델과 같은 장치(GPU)로 강제 이동\n",
        "    device = ldm_stable.device\n",
        "    start_noise = start_noise.to(device)\n",
        "\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # 2. 파라미터 설정 (Soft NTI + Relaxed P2P)\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # [수정됨] Cross Replace를 0.4로 낮추어 Identity 반영 구간 확보\n",
        "    cross_replace_steps = {\"default_\": 0.4}\n",
        "\n",
        "    # 텍스처(색감) 제약 완화 (0.2는 좋은 설정입니다)\n",
        "    self_replace_steps  = 0.2\n",
        "\n",
        "    # 배경 유지 강도 (0.6 ~ 0.7 추천)\n",
        "    nti_strength = 0.6\n",
        "\n",
        "    print(f\"Applying Soft NTI (Strength: {nti_strength}) using Slerp\")\n",
        "    print(f\"Relaxing P2P Constraints (Self-Replace: {self_replace_steps})\")\n",
        "\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # 3. Soft NTI Latent 생성 (Slerp 적용)\n",
        "    # ------------------------------------------------------------------------------\n",
        "    random_latents = torch.randn_like(start_noise)\n",
        "\n",
        "    # [수정됨] 단순 선형 합 대신 Slerp 사용\n",
        "    # nti_strength가 1에 가까울수록 start_noise(원본 구조)를 많이 가져감\n",
        "    # 따라서 random_latents 쪽으로 가는 비율은 (1 - nti_strength)가 됨\n",
        "    start_latents = slerp(1.0 - nti_strength, start_noise, random_latents)\n",
        "\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # 4. P2P 컨트롤러 설정\n",
        "    # ------------------------------------------------------------------------------\n",
        "    prompt_B = prompt_A.replace(source_word, placeholder_token)\n",
        "    prompts = [prompt_A, prompt_B]\n",
        "\n",
        "    # Attention Refinement 제어\n",
        "    controller = make_controller(\n",
        "        prompts,\n",
        "        True,\n",
        "        cross_replace_steps,\n",
        "        self_replace_steps,\n",
        "        blend_word,\n",
        "        None,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # 5. 이미지 생성\n",
        "    # ------------------------------------------------------------------------------\n",
        "    print(\"Generating image...\")\n",
        "\n",
        "    try:\n",
        "        # 주의: NTI의 uncond_embeddings가 너무 강력하면(Overfitting),\n",
        "        # Identity 변경을 방해할 수 있습니다. 여전히 안 바뀐다면\n",
        "        # uncond_embeddings 대신 ldm_stable.get_learned_conditioning([\"\"]) 사용 고려.\n",
        "        images_edit, _ = text2image_ldm_stable(\n",
        "            ldm_stable,\n",
        "            prompts,\n",
        "            controller,\n",
        "            num_inference_steps=NUM_DDIM_STEPS,\n",
        "            guidance_scale=GUIDANCE_SCALE,\n",
        "            generator=None,\n",
        "            latent=start_latents,\n",
        "            uncond_embeddings=uncond_embeddings,\n",
        "            start_time=NUM_DDIM_STEPS,\n",
        "            return_type=\"image\",\n",
        "        )\n",
        "        print(\"Generation finished.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation Failed with Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        images_edit = []\n",
        "\n",
        "    # ------------------------------------------------------------------------------\n",
        "    # 6. 결과 시각화\n",
        "    # ------------------------------------------------------------------------------\n",
        "    if images_edit is None or len(images_edit) == 0:\n",
        "        print(\"이미지가 생성되지 않았습니다.\")\n",
        "    else:\n",
        "        print(\"Displaying results...\")\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # (1) Original GT\n",
        "        plt.subplot(1, 3, 1)\n",
        "        if isinstance(image_gt, torch.Tensor):\n",
        "            # Tensor인 경우 permute 처리\n",
        "            disp_img = image_gt.detach().cpu()\n",
        "            if disp_img.ndim == 4: disp_img = disp_img[0]\n",
        "            plt.imshow(disp_img.permute(1, 2, 0).numpy())\n",
        "        else:\n",
        "            plt.imshow(image_gt)\n",
        "        plt.title(\"Original (A)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # (2) Result\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(images_edit[0])\n",
        "        plt.title(f\"Result (<sks-cat>)\\nSoft NTI: {nti_strength}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # (3) Reference B\n",
        "        if 'b_image_paths' in locals() and len(b_image_paths) > 0:\n",
        "            plt.subplot(1, 3, 3)\n",
        "            # 경로가 올바른지 확인 후 로드\n",
        "            try:\n",
        "                ref_img = Image.open(b_image_paths[0]).resize((512,512))\n",
        "                plt.imshow(ref_img)\n",
        "            except:\n",
        "                print(\"Reference image load failed.\")\n",
        "            plt.title(\"Reference (B)\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CmEO9b7U_S1g",
      "metadata": {
        "id": "CmEO9b7U_S1g"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# [실험 도구] 파라미터 Grid Search 및 비교 시각화\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# 1. 테스트하고 싶은 조합들을 리스트에 넣으세요.\n",
        "# (nti_strength, cross_replace, self_replace) 순서\n",
        "experiments = [\n",
        "    (0.7, 0.4, 0.2),  # 현재 설정 (기준)\n",
        "    (0.8, 0.7, 0.2),  # 배경/구도 강화\n",
        "    (0.9, 0.4, 0.3),  # 전체적인 보존력 강화\n",
        "    (0.9, 0.9, 0.3),  # 구조는 강하게, 텍스처는 유연하게\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"총 {len(experiments)}개의 조합을 실험합니다...\")\n",
        "\n",
        "for idx, (nti, cross, self_r) in enumerate(experiments):\n",
        "    print(f\"\\n[Experiment {idx+1}/{len(experiments)}] NTI:{nti}, Cross:{cross}, Self:{self_r}\")\n",
        "\n",
        "    # 1. Soft NTI (Slerp)\n",
        "    random_latents = torch.randn_like(start_noise)\n",
        "    start_latents = slerp(1.0 - nti, start_noise, random_latents)\n",
        "\n",
        "    # 2. P2P Controller\n",
        "    cross_replace_steps = {\"default_\": cross}\n",
        "    self_replace_steps = self_r\n",
        "\n",
        "    controller = make_controller(\n",
        "        prompts, True, cross_replace_steps, self_replace_steps, blend_word, None\n",
        "    )\n",
        "\n",
        "    # 3. Generation\n",
        "    try:\n",
        "        imgs, _ = text2image_ldm_stable(\n",
        "            ldm_stable, prompts, controller,\n",
        "            num_inference_steps=NUM_DDIM_STEPS,\n",
        "            guidance_scale=GUIDANCE_SCALE,\n",
        "            generator=None,\n",
        "            latent=start_latents,\n",
        "            uncond_embeddings=uncond_embeddings, # 필요시 ldm_stable.get_learned_conditioning([\"\"]) 로 변경 테스트\n",
        "            start_time=NUM_DDIM_STEPS,\n",
        "            return_type=\"image\"\n",
        "        )\n",
        "        results.append((imgs[0], f\"NTI:{nti}\\nCr:{cross}, Sf:{self_r}\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        results.append((np.zeros((512,512,3)), \"Error\"))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 결과 시각화 (한 줄에 쫙 펼쳐서 비교)\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# (1) 원본\n",
        "plt.subplot(1, len(experiments)+2, 1)\n",
        "if isinstance(image_gt, torch.Tensor):\n",
        "    disp_img = image_gt.detach().cpu()\n",
        "    if disp_img.ndim == 4: disp_img = disp_img[0]\n",
        "    plt.imshow(disp_img.permute(1, 2, 0).numpy())\n",
        "else:\n",
        "    plt.imshow(image_gt)\n",
        "plt.title(\"Original (A)\")\n",
        "plt.axis('off')\n",
        "\n",
        "# (2) 실험 결과들\n",
        "for i, (img, label) in enumerate(results):\n",
        "    plt.subplot(1, len(experiments)+2, i+2)\n",
        "    plt.imshow(img)\n",
        "    plt.title(label, fontsize=10)\n",
        "    plt.axis('off')\n",
        "\n",
        "# (3) 레퍼런스 B\n",
        "if 'b_image_paths' in locals() and len(b_image_paths) > 0:\n",
        "    plt.subplot(1, len(experiments)+2, len(experiments)+2)\n",
        "    try:\n",
        "        plt.imshow(Image.open(b_image_paths[0]).resize((512,512)))\n",
        "        plt.title(\"Ref (B)\")\n",
        "    except: pass\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rg_d0-3eZkfh",
      "metadata": {
        "id": "Rg_d0-3eZkfh"
      },
      "source": [
        "## 모듈화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.optim.adam import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. Helper Functions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def slerp(val, low, high):\n",
        "    \"\"\"구면 선형 보간 (Spherical Linear Interpolation)\"\"\"\n",
        "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos(torch.clamp((low_norm * high_norm).sum(1), -1, 1))\n",
        "    so = torch.sin(omega)\n",
        "\n",
        "    if so.abs().sum() < 1e-6:\n",
        "        return (1.0 - val) * low + val * high\n",
        "\n",
        "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + \\\n",
        "          (torch.sin(val * omega) / so).unsqueeze(1) * high\n",
        "    return res\n",
        "\n",
        "def reset_attention_hooks(model):\n",
        "    \"\"\"P2P Hook 제거 (메모리 누수 및 에러 방지)\"\"\"\n",
        "    for module in model.unet.modules():\n",
        "        if hasattr(module, \"_forward_hooks\"):\n",
        "            module._forward_hooks.clear()\n",
        "    try:\n",
        "        if hasattr(model.unet, \"set_default_attn_processor\"):\n",
        "            model.unet.set_default_attn_processor()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def get_automask_from_attention(model, tokenizer, latents, prompt, target_word_index, resolution=16):\n",
        "    \"\"\"크로스 어텐션 기반 자동 마스크 생성\"\"\"\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "\n",
        "    # P2P Controller 등록 (외부 라이브러리 의존)\n",
        "    controller = AttentionStore()\n",
        "    ptp_utils.register_attention_control(model, controller)\n",
        "\n",
        "    # Forward Pass\n",
        "    t = torch.tensor([400]).long()\n",
        "    # device mismatch 방지: 스케줄러엔 CPU, 모델엔 GPU\n",
        "    noisy_latents = model.scheduler.add_noise(latents, torch.randn_like(latents), t.to(\"cpu\"))\n",
        "\n",
        "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length,\n",
        "                           truncation=True, return_tensors=\"pt\")\n",
        "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.unet(noisy_latents, t.to(model.device), encoder_hidden_states=text_embeddings)\n",
        "\n",
        "    # Attention Map 추출\n",
        "    attention_maps = controller.get_average_attention()\n",
        "    target_map = None\n",
        "\n",
        "    for key in attention_maps:\n",
        "        if key == resolution:\n",
        "            attn = attention_maps[key]\n",
        "            target_map = attn[0].mean(0)[:, target_word_index]\n",
        "            break\n",
        "\n",
        "    if target_map is None:\n",
        "        target_map = list(attention_maps.values())[0][0].mean(0)[:, target_word_index]\n",
        "\n",
        "    res_int = int(np.sqrt(target_map.shape[0]))\n",
        "    target_map = target_map.view(1, 1, res_int, res_int)\n",
        "    mask = F.interpolate(target_map, size=(64, 64), mode='bilinear')\n",
        "    mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "    mask[mask < 0.3] = 0  # Hard threshold\n",
        "\n",
        "    controller.reset()\n",
        "\n",
        "    # U-Net 원상복구\n",
        "    for name, module in model.unet.named_modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = CrossAttention.forward.__get__(module, CrossAttention)\n",
        "\n",
        "    return mask.detach()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. Textual Inversion Function\n",
        "# -------------------------------------------------------------------------\n",
        "def train_textual_inversion_with_mask(\n",
        "    model, tokenizer, ref_image_path, placeholder_token, initializer_token,\n",
        "    prompt_template=\"a photo of a {}\", target_word_index=4, num_train_steps=500, lr=1e-3, device=\"cuda\"\n",
        "):\n",
        "    print(f\"\\n[Step 1] Training Textual Inversion for '{placeholder_token}'\")\n",
        "\n",
        "    # 이미지 로드 및 Latent 변환\n",
        "    img = load_512(ref_image_path)\n",
        "    latents_B = null_inversion.image2latent(img).to(device)\n",
        "\n",
        "    # 토큰 추가 및 초기화\n",
        "    num_added = tokenizer.add_tokens(placeholder_token)\n",
        "    placeholder_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "    init_id = tokenizer.convert_tokens_to_ids(initializer_token)\n",
        "\n",
        "    model.text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    token_embeds = model.text_encoder.get_input_embeddings().weight.data\n",
        "    token_embeds[placeholder_id] = token_embeds[init_id].clone()\n",
        "\n",
        "    # 마스크 생성\n",
        "    print(\"  Generating attention mask...\")\n",
        "    mask = get_automask_from_attention(\n",
        "        model, tokenizer, latents_B,\n",
        "        prompt=prompt_template.format(initializer_token),\n",
        "        target_word_index=target_word_index\n",
        "    ).detach().clone()\n",
        "\n",
        "    # 학습 설정\n",
        "    model.text_encoder.train()\n",
        "    model.text_encoder.get_input_embeddings().weight.requires_grad_(True)\n",
        "    optimizer = Adam([model.text_encoder.get_input_embeddings().weight], lr=lr)\n",
        "\n",
        "    train_prompt = prompt_template.format(placeholder_token)\n",
        "    text_input = tokenizer(train_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length,\n",
        "                           truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = text_input.input_ids.to(device)\n",
        "\n",
        "    # 학습 루프\n",
        "    print(f\"  Optimizing ({num_train_steps} Steps)...\")\n",
        "    latents_B = latents_B.detach()\n",
        "\n",
        "    for step in range(num_train_steps):\n",
        "        noise = torch.randn_like(latents_B)\n",
        "        bsz = latents_B.shape[0]\n",
        "        # Device Mismatch 해결\n",
        "        timesteps = torch.randint(0, model.scheduler.config.num_train_timesteps, (bsz,), device=\"cpu\").long()\n",
        "        noisy_latents = model.scheduler.add_noise(latents_B, noise, timesteps)\n",
        "\n",
        "        encoder_hidden_states = model.text_encoder(input_ids)[0]\n",
        "        noise_pred = model.unet(noisy_latents, timesteps.to(device), encoder_hidden_states).sample\n",
        "\n",
        "        loss_pixel = F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
        "        loss = (loss_pixel * mask).mean()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        grads = model.text_encoder.get_input_embeddings().weight.grad\n",
        "        if grads is not None:\n",
        "            target_grad = grads[placeholder_id, :].clone()\n",
        "            grads.data.zero_()\n",
        "            grads.data[placeholder_id, :] = target_grad\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"    Step {step}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Embedding optimized successfully!\")\n",
        "    return placeholder_id"
      ],
      "metadata": {
        "id": "CButvvA9eH0g"
      },
      "id": "CButvvA9eH0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# [설정] 이미지 및 프롬프트 경로\n",
        "# =============================================================================\n",
        "source_image_path = \"./car1.jpg\"      # A 이미지 (구조)\n",
        "reference_image_path = \"./car2.jpg\"    # B 이미지 (스타일/정체성)\n",
        "source_prompt = \"car standing on a beach\"\n",
        "source_word = \"car\"\n",
        "placeholder_token = \"<sks-car>\"\n",
        "\n",
        "# =============================================================================\n",
        "# [Step 1] Textual Inversion (정체성 학습)\n",
        "# =============================================================================\n",
        "reset_attention_hooks(ldm_stable)\n",
        "\n",
        "train_textual_inversion_with_mask(\n",
        "    model=ldm_stable,\n",
        "    tokenizer=tokenizer,\n",
        "    ref_image_path=reference_image_path,\n",
        "    placeholder_token=placeholder_token,\n",
        "    initializer_token=source_word,\n",
        "    num_train_steps=500,  # 충분히 학습\n",
        "    lr=1e-3,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# [Step 2] Null-text Inversion (원본 구조 추출)\n",
        "# =============================================================================\n",
        "print(f\"\\n[Step 2] Running Null-text Inversion on Source...\")\n",
        "\n",
        "(image_gt, image_rec), x_t, uncond_embeddings = null_inversion.invert(\n",
        "    source_image_path,\n",
        "    source_prompt,\n",
        "    offsets=(0, 0, 0, 0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"Inversion Completed.\")\n",
        "print(f\"   x_t shape: {x_t.shape}\")\n",
        "\n",
        "# 원본 재구성 확인\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1); plt.imshow(image_gt); plt.title(\"Original\"); plt.axis('off')\n",
        "plt.subplot(1, 2, 2); plt.imshow(image_rec); plt.title(\"Reconstruction\"); plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GvyhT5N2dlUL"
      },
      "id": "GvyhT5N2dlUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_nti_strength = 0.3      # 구조 유지 강도 (낮을수록 참조 이미지 형태 따라감)\n",
        "target_cross_replace = 0.7     # 교체 강도 (높을수록 참조 이미지 스타일 강함)\n",
        "target_self_replace = 0.3      # Self Attention 교체 강도\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 합성 단계 (Step 3) - 목표 파라미터로 이미지 생성\n",
        "# =============================================================================\n",
        "print(f\"\\n--- [합성 시작] NTI:{target_nti_strength}, Cross:{target_cross_replace}, Self:{target_self_replace} ---\")\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "    reset_attention_hooks(ldm_stable)\n",
        "\n",
        "    # (1) 프롬프트 설정\n",
        "    target_prompt = source_prompt.replace(source_word, placeholder_token)\n",
        "    prompts = [source_prompt, target_prompt]\n",
        "    blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "    # (2) Soft NTI (Slerp) 적용\n",
        "    random_noise = torch.randn_like(x_t)\n",
        "    start_latents = slerp(1.0 - target_nti_strength, x_t.flatten(1), random_noise.flatten(1))\n",
        "    start_latents = start_latents.view_as(x_t)\n",
        "\n",
        "    # (3) Controller 설정 (목표 파라미터 적용)\n",
        "    controller = make_controller(\n",
        "        prompts,\n",
        "        True, # is_replace_controller\n",
        "        {\"default_\": target_cross_replace},\n",
        "        target_self_replace,\n",
        "        blend_word,\n",
        "        None\n",
        "    )\n",
        "\n",
        "    # (4) 이미지 생성 실행\n",
        "    images_edit, _ = text2image_ldm_stable(\n",
        "        ldm_stable,\n",
        "        prompts,\n",
        "        controller,\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "        generator=None,\n",
        "        latent=start_latents,\n",
        "        uncond_embeddings=uncond_embeddings,\n",
        "        start_time=50,\n",
        "        return_type=\"image\",\n",
        "    )\n",
        "\n",
        "    result_image = images_edit[0]\n",
        "    print(\"합성 완료.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"합성 중 에러 발생: {e}\")\n",
        "    result_image = np.zeros((512, 512, 3)) # 에러 시 검은 이미지\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 결과 비교 시각화 (원본 vs 참조 vs 결과)\n",
        "# =============================================================================\n",
        "print(\"\\n--- 결과 시각화 ---\")\n",
        "\n",
        "# 원본 이미지 불러오기\n",
        "src_img_pil = Image.open(source_image_path).resize((512, 512))\n",
        "ref_img_pil = Image.open(reference_image_path).resize((512, 512))\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# [왼쪽] Source Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(src_img_pil)\n",
        "plt.title(\"Source (Car 1)\\nOriginal Structure\", fontsize=14, fontweight='bold')\n",
        "plt.axis('off')\n",
        "\n",
        "# [가운데] Reference Image\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(ref_img_pil)\n",
        "plt.title(\"Reference (Car 2)\\nTarget Style\", fontsize=14, fontweight='bold')\n",
        "plt.axis('off')\n",
        "\n",
        "# [오른쪽] Result Image\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(result_image)\n",
        "param_str = f\"N:{target_nti_strength}, C:{target_cross_replace}, S:{target_self_replace}\"\n",
        "plt.title(f\"Result (Synthesized)\\n{param_str}\", fontsize=14, fontweight='bold', color='blue')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZB2xIHnmDprN"
      },
      "id": "ZB2xIHnmDprN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# 실험 파라미터 설정 (여기만 수정해서 돌리세요)\n",
        "# =============================================================================\n",
        "params_to_test = []\n",
        "\n",
        "# 예: NTI(구조 유지력) 2개 x Cross(교체 강도) 5개 = 10개 조합\n",
        "nti_list = [0.3, 0.7]\n",
        "cross_list = [0.4,0.5, 0.6, 0.7, 0.8]\n",
        "fixed_self = 0.3                # Self Attention은 고정\n",
        "\n",
        "for nti in nti_list:\n",
        "    for cross in cross_list:\n",
        "        params_to_test.append((nti, cross, fixed_self))\n",
        "\n",
        "print(f\"Generating {len(params_to_test)} images...\")\n",
        "\n",
        "# =============================================================================\n",
        "# 이미지 생성 루프\n",
        "# =============================================================================\n",
        "results_images = []\n",
        "results_labels = []\n",
        "\n",
        "target_prompt = source_prompt.replace(source_word, placeholder_token)\n",
        "prompts = [source_prompt, target_prompt]\n",
        "blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "for idx, (nti, cross, self_rep) in enumerate(params_to_test):\n",
        "    print(f\"[{idx+1}/{len(params_to_test)}] NTI:{nti}, Cross:{cross} ...\", end=\"\")\n",
        "\n",
        "    try:\n",
        "        #메모리 확보 (런타임 끊김 방지)\n",
        "        torch.cuda.empty_cache()\n",
        "        reset_attention_hooks(ldm_stable)\n",
        "\n",
        "        # 1. Soft NTI (Slerp) - x_t 재사용\n",
        "        random_noise = torch.randn_like(x_t)\n",
        "        start_latents = slerp(1.0 - nti, x_t.flatten(1), random_noise.flatten(1))\n",
        "        start_latents = start_latents.view_as(x_t)\n",
        "\n",
        "        # 2. Controller 설정\n",
        "        controller = make_controller(\n",
        "            prompts,\n",
        "            True,\n",
        "            {\"default_\": cross},\n",
        "            self_rep,\n",
        "            blend_word,\n",
        "            None\n",
        "        )\n",
        "\n",
        "        # 3. 이미지 생성\n",
        "        images_edit, _ = text2image_ldm_stable(\n",
        "            ldm_stable,\n",
        "            prompts,\n",
        "            controller,\n",
        "            num_inference_steps=50,\n",
        "            guidance_scale=7.5,\n",
        "            generator=None,\n",
        "            latent=start_latents,\n",
        "            uncond_embeddings=uncond_embeddings, # Part 2에서 만든 것 재사용\n",
        "            start_time=50,\n",
        "            return_type=\"image\",\n",
        "        )\n",
        "\n",
        "        results_images.append(images_edit[0])\n",
        "        results_labels.append(f\"N:{nti}, C:{cross}\")\n",
        "        print(\" Done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {e}\")\n",
        "        results_images.append(np.zeros((512, 512, 3)))\n",
        "        results_labels.append(\"Error\")\n",
        "\n",
        "# =============================================================================\n",
        "# 결과 시각화\n",
        "# =============================================================================\n",
        "plt.figure(figsize=(20, 8))\n",
        "rows, cols = 2, 5  # 10개 기준\n",
        "\n",
        "for i in range(len(results_images)):\n",
        "    if i >= rows * cols: break\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(results_images[i])\n",
        "    ax.set_title(results_labels[i], fontsize=12, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YTN2oadfpguZ"
      },
      "id": "YTN2oadfpguZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5lzYV-zwbvvj",
      "metadata": {
        "id": "5lzYV-zwbvvj"
      },
      "source": [
        "## gird search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dCG_eO5ebwv5",
      "metadata": {
        "id": "dCG_eO5ebwv5"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Grid Search Pipeline for Hyperparameter Tuning\n",
        "# ============================================================================\n",
        "\n",
        "def identity_swapping_grid_search(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    null_inversion,\n",
        "    source_image_path: str,\n",
        "    reference_image_path: str,\n",
        "    source_prompt: str,\n",
        "    source_word: str,\n",
        "    placeholder_token: str = \"<sks-cat>\",\n",
        "    # Grid Search 파라미터\n",
        "    experiments: list = None,\n",
        "    ti_steps: int = 500,\n",
        "    ti_lr: float = 1e-3,\n",
        "    num_ddim_steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    device: str = \"cuda\"\n",
        "):\n",
        "    \"\"\"\n",
        "    여러 하이퍼파라미터 조합을 한 번에 테스트\n",
        "\n",
        "    Args:\n",
        "        experiments: 테스트할 (nti_strength, cross_replace, self_replace) 조합 리스트\n",
        "                    예: [(0.7, 0.4, 0.2), (0.8, 0.7, 0.2), ...]\n",
        "\n",
        "    Returns:\n",
        "        results: 각 실험 결과 이미지 리스트\n",
        "        image_gt: 원본 이미지\n",
        "        ref_image: 레퍼런스 이미지\n",
        "    \"\"\"\n",
        "\n",
        "    # 기본 실험 설정\n",
        "    if experiments is None:\n",
        "        experiments = [\n",
        "            (0.7, 0.4, 0.2),  # (NTI, Cross, Self)\n",
        "            (0.8, 0.7, 0.2),\n",
        "            (0.9, 0.4, 0.3),\n",
        "            (0.9, 0.9, 0.3),\n",
        "        ]\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"Grid Search Started\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"  Total Experiments: {len(experiments)}\")\n",
        "    print(f\"  Source (A): {source_image_path}\")\n",
        "    print(f\"  Reference (B): {reference_image_path}\")\n",
        "    print()\n",
        "\n",
        "    reset_attention_hooks(model)\n",
        "\n",
        "    # ===================================================================\n",
        "    # Step 1: Textual Inversion (한 번만 수행)\n",
        "    # ===================================================================\n",
        "    print(\"[Step 1] Training Textual Inversion (Once)\")\n",
        "    train_textual_inversion_with_mask(\n",
        "        model, tokenizer, reference_image_path,\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=source_word,\n",
        "        num_train_steps=ti_steps,\n",
        "        lr=ti_lr,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # ===================================================================\n",
        "    # Step 2: Null-text Inversion (한 번만 수행)\n",
        "    # ===================================================================\n",
        "    print(f\"\\n[Step 2] Null-text Inversion on Source Image\")\n",
        "    (image_gt, image_rec), x_t, uncond_embeddings = null_inversion.invert(\n",
        "        source_image_path,\n",
        "        source_prompt,\n",
        "        offsets=(0, 0, 0, 0),\n",
        "        verbose=False\n",
        "    )\n",
        "    print(f\"Inversion completed\")\n",
        "\n",
        "    # 레퍼런스 이미지 로드\n",
        "    ref_image = Image.open(reference_image_path).convert(\"RGB\").resize((512, 512))\n",
        "    ref_image = np.array(ref_image)\n",
        "\n",
        "    # ===================================================================\n",
        "    # Step 3: 각 하이퍼파라미터 조합에 대해 이미지 생성\n",
        "    # ===================================================================\n",
        "    results = []\n",
        "    target_prompt = source_prompt.replace(source_word, placeholder_token)\n",
        "\n",
        "    print(f\"\\n[Step 3] Running {len(experiments)} Experiments\")\n",
        "    print(f\"  Source Prompt: '{source_prompt}'\")\n",
        "    print(f\"  Target Prompt: '{target_prompt}'\")\n",
        "    print()\n",
        "\n",
        "    for idx, (nti_strength, cross_replace, self_replace) in enumerate(experiments, 1):\n",
        "        print(f\"  [{idx}/{len(experiments)}] NTI:{nti_strength}, Cross:{cross_replace}, Self:{self_replace}\")\n",
        "\n",
        "        try:\n",
        "            # Soft NTI\n",
        "            random_noise = torch.randn_like(x_t)\n",
        "            start_latents = slerp(1.0 - nti_strength, x_t.flatten(1), random_noise.flatten(1))\n",
        "            start_latents = start_latents.view_as(x_t)\n",
        "\n",
        "            # P2P Controller\n",
        "            prompts = [source_prompt, target_prompt]\n",
        "            blend_word = (((source_word,), (placeholder_token,)),)\n",
        "\n",
        "            controller = make_controller(\n",
        "                prompts,\n",
        "                True,\n",
        "                {\"default_\": cross_replace},\n",
        "                self_replace,\n",
        "                blend_word,\n",
        "                None\n",
        "            )\n",
        "\n",
        "            # 이미지 생성\n",
        "            images_edit, _ = text2image_ldm_stable(\n",
        "                model,\n",
        "                prompts,\n",
        "                controller,\n",
        "                num_inference_steps=num_ddim_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                generator=None,\n",
        "                latent=start_latents,\n",
        "                uncond_embeddings=uncond_embeddings,\n",
        "                start_time=num_ddim_steps,\n",
        "                return_type=\"image\",\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'image': images_edit[0],\n",
        "                'params': f\"NTI:{nti_strength}\\nCr:{cross_replace}, Sf:{self_replace}\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            results.append({\n",
        "                'image': np.zeros((512, 512, 3), dtype=np.uint8),\n",
        "                'params': \"Error\"\n",
        "            })\n",
        "\n",
        "    reset_attention_hooks(model)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Grid Search Completed!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return results, image_gt, ref_image\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 결과 시각화 함수\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_grid_results(results, image_gt, ref_image, figsize=(20, 6)):\n",
        "    \"\"\"\n",
        "    Grid Search 결과를 한 줄로 시각화\n",
        "\n",
        "    Layout: [원본] [실험1] [실험2] ... [실험N] [레퍼런스]\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    num_experiments = len(results)\n",
        "    total_images = num_experiments + 2  # 원본 + 실험들 + 레퍼런스\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # (1) 원본 이미지\n",
        "    plt.subplot(1, total_images, 1)\n",
        "    plt.imshow(image_gt)\n",
        "    plt.title(\"Original (A)\", fontsize=10, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # (2) 실험 결과들\n",
        "    for i, result in enumerate(results, 2):\n",
        "        plt.subplot(1, total_images, i)\n",
        "        plt.imshow(result['image'])\n",
        "        plt.title(result['params'], fontsize=9)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # (3) 레퍼런스 이미지\n",
        "    plt.subplot(1, total_images, total_images)\n",
        "    plt.imshow(ref_image)\n",
        "    plt.title(\"Ref (B)\", fontsize=10, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bwmZ5F_EdKX4",
      "metadata": {
        "id": "bwmZ5F_EdKX4"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 실행 예시\n",
        "# ============================================================================\n",
        "\n",
        "# 실험할 하이퍼파라미터 조합들 정의\n",
        "experiments = [\n",
        "    (0.7, 0.4, 0.2),  # 현재 설정 (기준)\n",
        "    (0.8, 0.7, 0.2),  # 배경/구도 강화\n",
        "    (0.9, 0.4, 0.3),  # 전체적인 보존력 강화\n",
        "    (0.9, 0.9, 0.3),  # 구조는 강하게, 텍스처는 유연하게\n",
        "]\n",
        "\n",
        "# Grid Search 실행\n",
        "results, gt, ref = identity_swapping_grid_search(\n",
        "    model=ldm_stable,\n",
        "    tokenizer=tokenizer,\n",
        "    null_inversion=null_inversion,\n",
        "    source_image_path=\"./gnochi_mirror.jpeg\",\n",
        "    reference_image_path=\"./cat.jpg\",\n",
        "    source_prompt=\"a cat sitting next to a mirror\",\n",
        "    source_word=\"cat\",\n",
        "    placeholder_token=\"<sks-cat>\",\n",
        "    experiments=experiments,\n",
        "    ti_steps=500,\n",
        ")\n",
        "\n",
        "# 결과 시각화\n",
        "visualize_grid_results(results, gt, ref, figsize=(20, 6))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "pytorch-gpu.1-12.m97",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}